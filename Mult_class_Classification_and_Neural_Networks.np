import numpy as np
import scipy.io as sio
from matplotlib import pyplot as plt
from scipy.optimize import minimize
data=sio.loadmat("ex3data1.mat")
X=data["X"]
y=data["y"].ravel()
import random,PIL.Image as Image
num_sample=100
samples=random.sample(list(X),num_sample)
display_img=Image.new("L",(200,200))
i=0
for col in range(10):
    for row in range(10):
        img_arr=(samples[i].reshape(20,20).T*255).astype(np.uint8)
        display_img.paste(Image.fromarray(img_arr),(col*20,row*20))
        i+=1
plt.imshow(display_img,cmap='gray');plt.show()
X=np.hstack([np.ones((X.shape[0],1)),X])
def sigmoid(z): return 1/(1+np.exp(-z))

def compute_regularized_cost(theta, X, y, _lambda):
    m = len(y)
    reg = _lambda/(2*m)*np.sum(theta[1:]**2)
    cost = -y@np.log(sigmoid(X@theta)) - (1-y)@np.log(1-sigmoid(X@theta))
    return (cost/m) + reg

def compute_regularized_gradient(theta, X, y, _lambda):
    m = len(y)
    grad = (X.T @ (sigmoid(X@theta) - y)) / m
    reg = (_lambda/m)*theta
    reg[0] = 0
    return grad + reg
def train(X,y,K,lamda):
    n=X.shape[1]
    all_theta=np.zeros((K,n))
    for k in range(K):
        y_k=(y==k+1).astype(int)
        res=minimize(compute_regularized_cost,np.zeros(n),args=(X,y_k,lamda),method="CG",jac=compute_regularized_gradient)
        all_theta[k]=res.x
    return all_theta
K=10
lamda=0.1
theta=train(X,y,K,lamda)
pred=np.argmax(sigmoid(X@theta.T),axis=1)+1
accuracy=np.mean(pred==y)*100
print(accuracy)
weights=sio.loadmat("ex3weights.mat")
theta1=weights["Theta1"]
theta2=weights["Theta2"]
def add_bias(X):
    return np.hstack([np.ones((X.shape[0],1)),X])
def forward(theta1,theta2,X):
    a1=X
    z2=a1@theta1.T
    a2=sigmoid(z2)
    a2=add_bias(a2)
    z3=a2@theta2.T
    a3=sigmoid(z3)
    return a3
pred_NN=np.argmax(forward(theta1,theta2,X),axis=1)+1
accuracy_NN=np.mean(pred_NN==y)*100
print(accuracy_NN)
